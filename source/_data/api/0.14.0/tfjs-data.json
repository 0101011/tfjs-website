{
  "docs": {
    "headings": [{
      "name": "Data",
      "description": "",
      "subheadings": [{
          "name": "Classes",
          "symbols": [{
              "docInfo": {
                "heading": "Data",
                "subheading": "Classes"
              },
              "symbolName": "Dataset",
              "documentation": "Represents a potentially large set of elements.\n\nA `Dataset` can be used to represent an input pipeline as a\ncollection of elements (maps from string keys to values) and a \"logical\nplan\" of transformations that act on those elements.\n\nA `Dataset` provides a stream of unbatched examples, and its transformations\nare applied one example at a time.  Batching produces a BatchDataset, and so\nmust come last in the pipeline because there are (so far) no batch-enabled\ntransformations.",
              "fileName": "#43",
              "githubUrl": "https://github.com/tensorflow/tfjs-data/blob/v0.1.2/src/dataset.ts#L43-L268",
              "methods": [],
              "isClass": true
            },
            {
              "docInfo": {
                "heading": "Data",
                "subheading": "Classes"
              },
              "symbolName": "CSVDataset",
              "documentation": "Represents a potentially large collection of delimited text records.\n\nThe produced `DataElement`s each contain one key-value pair for\nevery column of the table.  When a field is empty in the incoming data, the\nresulting value is `undefined`, or throw error if it is required.  Values\nthat can be parsed as numbers are emitted as type `number`, other values\nare parsed as `string`.\n\nThe results are not batched.",
              "fileName": "#47",
              "githubUrl": "https://github.com/tensorflow/tfjs-data/blob/v0.1.2/src/datasets/csv_dataset.ts#L47-L362",
              "methods": [],
              "isClass": true,
              "inheritsFrom": "Dataset"
            }
          ]
        },
        {
          "name": "Creation",
          "symbols": [{
            "docInfo": {
              "heading": "Data",
              "subheading": "Creation"
            },
            "symbolName": "array",
            "paramStr": "(items)",
            "parameters": [{
              "name": "items",
              "documentation": "An array of elements that will be parsed as items in a dataset.",
              "type": "DataElement[]",
              "optional": false,
              "isConfigParam": false
            }],
            "returnType": "Dataset",
            "documentation": "Create a `Dataset` from an array of elements.\n\n```js\nconst a = tf.data.array([{'item': 1}, {'item': 2}, {'item': 3}]);\nawait a.forEach(e => console.log(JSON.stringify(e)));\n\nconst b = tf.data.array([4, 5, 6]);\nawait b.forEach(e => console.log(JSON.stringify(e)));\n```",
            "fileName": "#300",
            "githubUrl": "https://github.com/tensorflow/tfjs-data/blob/v0.1.2/src/dataset.ts#L300-L302",
            "isFunction": true
          }]
        },
        {
          "name": "Operations",
          "symbols": [{
            "docInfo": {
              "heading": "Data",
              "subheading": "Operations"
            },
            "symbolName": "zip",
            "paramStr": "(datasets)",
            "parameters": [{
              "name": "datasets",
              "documentation": "",
              "type": "DatasetContainer",
              "optional": false,
              "isConfigParam": false
            }],
            "returnType": "Dataset",
            "documentation": "Create a `Dataset` by zipping together an array, dict, or nested\nstructure of `Dataset`s (and perhaps additional constants).\nThe underlying datasets must provide elements in a consistent order such that\nthey correspond.\n\nThe number of elements in the resulting dataset is the same as the size of\nthe smallest dataset in `datasets`.\n\nThe nested structure of the `datasets` argument determines the\nstructure of elements in the resulting iterator.\n\nNote this means that, given an array of two datasets that produce dict\nelements, the result is a dataset that produces elements that are arrays\nof two dicts:\n\n```js\nconst ds1 = tf.data.array([{a: 1}, {a: 2}, {a: 3}]);\nconst ds2 = tf.data.array([{b: 4}, {b: 5}, {b: 6}]);\nconst ds3 = tf.data.zip([ds1, ds2]);\nawait ds3.forEach(e => console.log(JSON.stringify(e)));\n\n// If the goal is to merge the dicts in order to produce elements like\n// {a: ..., b: ...}, this requires a second step such as:\nconst ds4 = ds3.map(x => {return {a: x[0].a, b: x[1].b}});\nawait ds4.forEach(e => console.log(JSON.stringify(e)));\n```",
            "fileName": "#333",
            "githubUrl": "https://github.com/tensorflow/tfjs-data/blob/v0.1.2/src/dataset.ts#L333-L353",
            "isFunction": true
          }]
        },
        {
          "name": "Sources",
          "symbols": [{
              "docInfo": {
                "heading": "Data",
                "subheading": "Sources"
              },
              "symbolName": "URLDataSource",
              "documentation": "",
              "fileName": "#28",
              "githubUrl": "https://github.com/tensorflow/tfjs-data/blob/v0.1.2/src/sources/url_data_source.ts#L28-L49",
              "methods": [],
              "isClass": true,
              "inheritsFrom": "DataSource"
            },
            {
              "docInfo": {
                "heading": "Data",
                "subheading": "Sources"
              },
              "symbolName": "FileDataSource",
              "documentation": "Represents a file, blob, or Uint8Array readable as a stream of binary data\nchunks.",
              "fileName": "#29",
              "githubUrl": "https://github.com/tensorflow/tfjs-data/blob/v0.1.2/src/sources/file_data_source.ts#L29-L46",
              "methods": [],
              "isClass": true,
              "inheritsFrom": "DataSource"
            }
          ]
        }
      ]
    }]
  },
  "docLinkAliases": {}
}
